<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[My Octopress Blog]]></title>
  <link href="http://parkr.github.com/info4300-notes/atom.xml" rel="self"/>
  <link href="http://parkr.github.com/info4300-notes/"/>
  <updated>2012-11-01T12:26:59-04:00</updated>
  <id>http://parkr.github.com/info4300-notes/</id>
  <author>
    <name><![CDATA[Your Name]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DFS &amp; MapReduce]]></title>
    <link href="http://parkr.github.com/info4300-notes/blog/2012/11/01/dfs-mapreduce/"/>
    <updated>2012-11-01T12:08:00-04:00</updated>
    <id>http://parkr.github.com/info4300-notes/blog/2012/11/01/dfs-mapreduce</id>
    <content type="html"><![CDATA[<h3>Hadoop</h3>

<p>Hadoop is an open-source implementation that combines MapReduce and Distributed File-System technologies.</p>

<p>Useful for the indexing side of web searching.</p>

<ul>
<li>Designed for hundreds of thousands of server machines</li>
<li>Add or remove physical machines at any time</li>
<li>Automatic detection of faults (with quick, automatic recover)</li>
<li>Optimized for larger files, not many small files.</li>
<li>Supports write-once-read-many access model first and foremost</li>
<li>Automatic replication (~3 times, usually)</li>
<li>No other backup!</li>
</ul>


<p>Hadoop is used by many large, well-known companies. It is believed that Facebook uses Hadoop.</p>

<h3>GFS &amp; HFS</h3>

<h4>Literature</h4>

<ul>
<li><a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/gfs-sosp2003.pdf">GFS</a></li>
</ul>


<h4>Overview</h4>

<ul>
<li>Built from many inexpensive components that often fail.</li>
<li>Optimized for 100MB+ files</li>
<li>High sustained bandwidth is more important than low latency</li>
</ul>


<h4>Workload</h4>

<ul>
<li>Two kinds of reads are most common:

<ul>
<li>large streamin reads</li>
<li>small random reads</li>
</ul>
</li>
<li>many large, sequential writes that append data to files</li>
<li>once written, files are seldom modified again</li>
</ul>


<h4>Architecture</h4>

<ul>
<li>Cluster

<ul>
<li>contains a single <strong>master</strong> and several <strong>chunkservers</strong></li>
<li>clients and chunkservers can be run on the same computers</li>
</ul>
</li>
<li>Files divided into fixed-sized <strong>chunks</strong>. Each chunk has a unique 64-bit <strong>chunk handle</strong> assigned by master server</li>
<li>Chunkservers store the chunks on local disks as linux files, and read &amp; write</li>
</ul>


<h4>Client interactions</h4>

<ul>
<li>A client asks the master which chunkservers it should contact and completes the file exchange with that chunkserver directly (not via master)</li>
</ul>


<h3>Programming for Large Computer Clusters</h3>

<h4>Goal</h4>

<ul>
<li>Create env in which less-experienced programmers could develop useful network services quickly</li>
<li>Separate app programmers from the complexities of the DFS</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[new-post]]></title>
    <link href="http://parkr.github.com/info4300-notes/blog/2012/11/01/new-post/"/>
    <updated>2012-11-01T12:06:00-04:00</updated>
    <id>http://parkr.github.com/info4300-notes/blog/2012/11/01/new-post</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
</feed>
