<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Information Retrieval]]></title>
  <link href="http://info4300.notes.parkermoore.de/atom.xml" rel="self"/>
  <link href="http://info4300.notes.parkermoore.de/"/>
  <updated>2012-11-06T13:53:01-05:00</updated>
  <id>http://info4300.notes.parkermoore.de/</id>
  <author>
    <name><![CDATA[Parker J. Moore, pjm336[at]cornell.edu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Users and Usability]]></title>
    <link href="http://info4300.notes.parkermoore.de/2012/11/06/users-and-usability/"/>
    <updated>2012-11-06T11:48:00-05:00</updated>
    <id>http://info4300.notes.parkermoore.de/2012/11/06/users-and-usability</id>
    <content type="html"><![CDATA[<ul>
<li><strong>Users are extremely variable</strong></li>
<li><strong>Users are not search professionals</strong></li>
</ul>


<h3>Berry Picking</h3>

<ul>
<li>Conventional information retrieval assumes that the user starts with a well-formulated information need and attempt to satisfy that need by a combination of searching and browsing.</li>
<li>In many practical situations, the user begins by exploring a topic.  This exploration leads to greater understanding of the topic, which may eventually lead to a well formulated information need, or may lead the user in directions that were not anticipated at the beginning.</li>
</ul>


<h2>Web Search: Browsing</h2>

<ul>
<li>Users give queries of 2 to 4 words.</li>
<li>Most users click on on the first few results. Few go beyond the fold on the first page.</li>
<li>80% of users, use search engine to find sites:

<ul>
<li><strong>search</strong> to find site</li>
<li><strong>browse</strong> the site to find information</li>
</ul>
</li>
</ul>


<p><em>Amil Singal, Google, 2004</em></p>

<h3>Browsing</h3>

<p>If docs are accessible online, user can browse content</p>

<ul>
<li>Inspecting docs can compensate for weaknesses in the underlying search system, e.g., the difficulty of ranking web docs</li>
<li>&#8230; but requires rapid delivery to the desktop.</li>
</ul>


<p>Otherwise, the user can browse surrogates, e.g., sets of catalog records, subject hierarchies, cluters of documents, etc.</p>

<ul>
<li>Browsing surrogates puts heavy demands on the underlying search system.</li>
<li>Surrogates heavily used in library catalogs</li>
</ul>


<h4>Browsing by filtering &amp; sorting</h4>

<ul>
<li><strong>Filters</strong> are used to extract a small subset from a large collection. Allow users to reject categories</li>
<li><strong>Sorting</strong> subsets in various ways allows users to organize data for rapid scanning &amp; quick retrieval</li>
</ul>


<h4>Snippets</h4>

<ul>
<li>A snippet is a short record that a search system returns to describe a hit and provide a link to an online document.</li>
<li>Good snippet design affects usability.</li>
<li>Variation of snippet choices:

<ul>
<li>Dynamic or static (pre-computed) snippet?</li>
<li>Content only or with related information?</li>
<li>highlighting the search terms</li>
<li>balance b/w length of snippet and # of snippets on the page</li>
</ul>
</li>
<li>Snippets help users understand why the hit was included in the results</li>
</ul>


<h2>Designing the Search Page</h2>

<ul>
<li>Overall organization

<ul>
<li>Spacious or cramped</li>
<li>Page layout showing division of functionality</li>
<li>Positioning components in the interface</li>
<li>Emphasizing parts of the interface</li>
</ul>
</li>
<li>Queries

<ul>
<li>insert text string or fill in text boxes</li>
<li>Auto complete, suggested queries, spelling correction, etc.</li>
</ul>
</li>
<li>Interactivity</li>
<li>Performance requirements</li>
</ul>


<h3>The Yahoo! Interface</h3>

<ul>
<li>though incredibly cluttered and unattractive, was somewhat effective</li>
<li>very many brnches from a single web page saved the need for hierarchy of menus</li>
<li>simple HTML markup ensured quickness &amp; accuracy on all browsers</li>
<li>didn&#8217;t change often, so familiar users could get what they wanted very quickly</li>
</ul>


<h3>Design/Evaluate Process</h3>

<ol>
<li>Requirements</li>
<li>Design</li>
<li>Implementation</li>
<li>Evaluation</li>
</ol>


<h3>User Interface</h3>

<ul>
<li>Interface (visual elements)</li>
<li>Functions (fields, content, operators, etc)</li>
<li>Data (metadata, data/file structures)</li>
<li>Systems (performance)</li>
<li><strong>Usability</strong> is paramount</li>
</ul>


<h3>Evaluation</h3>

<p>Process of determining the worth of, or assinging a value to, the usability on the basis of careful examination and judgement.</p>

<ul>
<li>Categories of evaluation methods

<ul>
<li>analytical evaluation w/o users</li>
<li>empirical evaluation with users</li>
<li>measurements of operational systems</li>
</ul>
</li>
</ul>


<h4>Usability</h4>

<p>Usability has the following aspects:</p>

<ol>
<li>Effectiveness

<ul>
<li>The accuracy and completeness with which users achieve certain goals.</li>
<li><strong>Measures:</strong> quality of solution, error rates</li>
</ul>
</li>
<li>Efficiency

<ul>
<li>The relation between the effectiveness and the resources expended in achieving them</li>
<li><strong>Measures:</strong> task completion time, learning time, number of clicks</li>
</ul>
</li>
<li>Satisfaction

<ul>
<li>The users&#8217; comfort with and positive attitudes towards the use of the system</li>
<li><strong>Measures:</strong> attitude rating scales</li>
</ul>
</li>
</ol>


<h4>Measurements of Operational Systems</h4>

<ul>
<li>Analysis of system logs

<ul>
<li>Which user interface options were used?</li>
<li>When was was the help system used?</li>
<li>What errors occurred and how often?</li>
<li>Which hyperlinks were followed (click through data)?</li>
</ul>
</li>
<li>Human feedback

<ul>
<li>Complaints and praise</li>
<li>Bug reports</li>
<li>Requests made to customer service</li>
</ul>
</li>
</ul>


<h4>Experiments on Op. Systems</h4>

<ol>
<li>Change some small part of the system for a small # of users</li>
<li>Measure effect of change</li>
<li>Is the effect of change positive? Negative? Negligible?</li>
</ol>


<h4>User Testing</h4>

<ol>
<li>Preparation (what is being evaluated?)</li>
<li>Conduct sessions w/ users</li>
<li>Analysis of results</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DFS &amp; MapReduce]]></title>
    <link href="http://info4300.notes.parkermoore.de/2012/11/01/dfs-mapreduce/"/>
    <updated>2012-11-01T12:08:00-04:00</updated>
    <id>http://info4300.notes.parkermoore.de/2012/11/01/dfs-mapreduce</id>
    <content type="html"><![CDATA[<h3>Hadoop</h3>

<p>Hadoop is an open-source implementation that combines MapReduce and Distributed File-System technologies.</p>

<p>Useful for the indexing side of web searching.</p>

<ul>
<li>Designed for hundreds of thousands of server machines</li>
<li>Add or remove physical machines at any time</li>
<li>Automatic detection of faults (with quick, automatic recover)</li>
<li>Optimized for larger files, not many small files.</li>
<li>Supports write-once-read-many access model first and foremost</li>
<li>Automatic replication (~3 times, usually)</li>
<li>No other backup!</li>
</ul>


<p>Hadoop is used by many large, well-known companies. It is believed that Facebook uses Hadoop.</p>

<h3>GFS &amp; HFS</h3>

<h4>Literature</h4>

<ul>
<li><a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/gfs-sosp2003.pdf">GFS</a></li>
</ul>


<h4>Overview</h4>

<ul>
<li>Built from many inexpensive components that often fail.</li>
<li>Optimized for 100MB+ files</li>
<li>High sustained bandwidth is more important than low latency</li>
</ul>


<h4>Workload</h4>

<ul>
<li>Two kinds of reads are most common:

<ul>
<li>large streamin reads</li>
<li>small random reads</li>
</ul>
</li>
<li>many large, sequential writes that append data to files</li>
<li>once written, files are seldom modified again</li>
</ul>


<h4>Architecture</h4>

<ul>
<li>Cluster

<ul>
<li>contains a single <strong>master</strong> and several <strong>chunkservers</strong></li>
<li>clients and chunkservers can be run on the same computers</li>
</ul>
</li>
<li>Files divided into fixed-sized <strong>chunks</strong>. Each chunk has a unique 64-bit <strong>chunk handle</strong> assigned by master server</li>
<li>Chunkservers store the chunks on local disks as linux files, and read &amp; write</li>
</ul>


<h4>Client interactions</h4>

<ul>
<li>A client asks the master which chunkservers it should contact and completes the file exchange with that chunkserver directly (not via master)</li>
</ul>


<h3>Programming for Large Computer Clusters</h3>

<h4>Goal</h4>

<ul>
<li>Create env in which less-experienced programmers could develop useful network services quickly</li>
<li>Separate app programmers from the complexities of the DFS</li>
</ul>


<h4>Academic researcher programmers</h4>

<ul>
<li>Deep knowledge of domain</li>
<li>Good algorithmic understanding</li>
</ul>


<p>BUT&#8230;</p>

<ul>
<li>Has limited understanding of large-scale data analysis</li>
<li>not skilled in any form of concurrent computing</li>
</ul>


<h4>Tape-Based Computing</h4>

<h5>Problem:</h5>

<ul>
<li>Waaaaay too much data to build data structures in memory</li>
<li>Disk I/O waaaay to slow for random access</li>
</ul>


<h5>Solution</h5>

<ul>
<li>Data managed by short key-value pairs</li>
<li>To process data:

<ul>
<li>split data into key-value pairs</li>
<li>sort data by key</li>
<li>merge files so that all values wht the same key are processed together</li>
<li>proess all the values for a single key</li>
<li>write out 1+ records for each key</li>
</ul>
</li>
</ul>


<h3>MapReduce</h3>

<h4>Programming</h4>

<ul>
<li><strong>Map</strong> takes an input pair and produces a set of intermediate key-value pairs</li>
<li><strong>Combiner</strong> merges all intermediate values associated with the saame intermediate key and passes them to Reduce</li>
<li><strong>Reduce</strong> accepts an intermediate key and a set of values for that key, combines these values to form a possibly smaller set of values</li>
</ul>


<h5>Map</h5>

<ul>
<li>Input: <code>(u0, v0)</code></li>
<li>Output:

<ul>
<li><code>(u, d)</code> &mdash; Dummy <code>d</code> indicates that <code>u</code> is a from-URL</li>
<li><code>(v, u)</code> &mdash; Indicate that <code>v</code> is a to-URL with link from <code>u</code></li>
<li><code>d</code> is a dummy marker.  Do not output if <code>u</code> = <code>v</code>.</li>
</ul>
</li>
<li>The output is a set of keys, which are all the URLs, with values that depend on whether they are from-URLs or to-URLs.</li>
</ul>


<h5>Combiner</h5>

<ul>
<li>The input to the reduce process merges the output values from the map task that correspond to each key (URL).</li>
<li>It combines all the (key, value) pairs from Map that have the same key.  For each URL, <code>w</code>, it creates a list:

<ul>
<li><code>w, {d, ... , d, u1, ..., uk}</code></li>
</ul>
</li>
<li>The Combine operation is performed automatically by the system libraries.</li>
</ul>


<h5>Reduce</h5>

<ul>
<li>Input: <code>(w, {d, ..., d, u1, ..., uk})</code>, where <code>w</code> is any URL.</li>
<li>Output:

<ul>
<li>If d does not exist, discard and do not output (corresponds to a URL that never appears as the first element of a (u, v) pair)</li>
<li>Otherwise, remove duplicates from <code>u1, ..., uk</code> and output.</li>
</ul>
</li>
<li>The output is a to-URL and a list of the nodes that link to it: <code>v, {u1, ..., uk}</code></li>
</ul>


<h4>Cluster Implementation</h4>

<p><img src="http://info4300.notes.parkermoore.de/images/mapreduce-implementation.png" title="&#34;Visual representation of MapReduce&#34;" alt="&#34;Visual representation of MapReduce&#34;"></p>

<h3>Scalability</h3>

<ul>
<li>Web search services are monolithic centralized systems (though use distributed data centers)</li>
<li>Moore&#8217;s Law allows for scalability on a grand scale</li>
<li>Harder to maintain quality of software when more and more people work on the same code base</li>
<li>Solution: MODULAR DEVELOPMENT</li>
<li>Organize for minimal staff in systems management as well as customer service</li>
<li>AUTOMATE AUTOMATE AUTOMATE</li>
</ul>

]]></content>
  </entry>
  
</feed>
